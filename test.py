from collections import Counter

from Config import Config
from process.Predictor import Predictor

from Config import Config
label_counter = Counter()
config = Config()
with open(config.path_dataset + "train.txt", "r", encoding="utf-8") as f:
    for line in f:
        if line.strip():
            _, label = line.strip().split()
            label_counter[label] += 1
print(label_counter)
#
# ALLOWED_DATASETS = ['CNER', 'CLUENER', 'CMEEE']
# ALLOWED_MODELS = ['bert_crf', 'bilstm_crf', 'roberta_crf']
# config = Config()
# config.dataset = "CMEEE"
# config.model_name = "roberta_crf"
# config.update_paths()
# predictor = Predictor(config,test_loader = None)
# """
# return {
#     'tokens': cleaned_tokens,
#     'labels': cleaned_labels,
#     'entities': bio_entities
# }
# """
# res = predictor.predict_text("å¤§é‡ç»™äºˆè‚¾ä¸Šè…ºçš®è´¨æ¿€ç´ è¿˜å¯èƒ½åŠ é‡æœºä½“çš„åº”æ¿€çŠ¶æ€ï¼Œä¹Ÿä¼šé€ æˆä¸¥é‡çš„ç»§å‘æ€§æ„ŸæŸ“ä½¿ç—…æƒ…åŠ é‡ã€‚")
# # æ£€æŸ¥æ ‡ç­¾å¯¹é½
#
# print("ğŸŸ¢ Tokens:")
# print(res["tokens"])
#
# print("\nğŸ”µ Labels:")
# print(res["labels"])
#
# print("\nğŸŸ£ Entities:")
# for entity in res["entities"]:
#     print(entity)
