from collections import Counter

from Config import Config
from process.Predictor import Predictor

# from Config import Config
# label_counter = Counter()
# config = Config()
# with open(config.path_dataset + "train.txt", "r", encoding="utf-8") as f:
#     for line in f:
#         if line.strip():
#             _, label = line.strip().split()
#             label_counter[label] += 1
# print(label_counter)
config = Config()
predictor = Predictor(config,test_loader = None)
"""
return {
    'tokens': cleaned_tokens,
    'labels': cleaned_labels,
    'entities': bio_entities
}
"""
res = predictor.predict_text("2009å¹´11æœˆè‡³2012å¹´5æœˆï¼Œä»»æµ™æ±Ÿæ˜ç‰Œç å®è‚¡ä»½æœ‰é™å…¬å¸è‘£äº‹ä¼šç§˜ä¹¦ï¼›")
# æ£€æŸ¥æ ‡ç­¾å¯¹é½

print("ğŸŸ¢ Tokens:")
print(res["tokens"])

print("\nğŸ”µ Labels:")
print(res["labels"])

print("\nğŸŸ£ Entities:")
for entity in res["entities"]:
    print(entity)
